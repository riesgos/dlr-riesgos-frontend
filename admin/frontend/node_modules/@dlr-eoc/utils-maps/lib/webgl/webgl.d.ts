/**
 * WEBGL
 *
 * A rasterization engine that allows to draw points, line segments, or triangles.
 *
 * Vertex shaders take whatever coordinates you use and return a 3-d array with elements between -1 and 1.
 * Basically, this is a 3d-array, but WebGl does not use the z-axis for real perspective, but only to differentiate
 * what pixel lies in front of another.
 * This is not like looking in a 3d-box, but rather like looking on multiple stacked sheets on a projector.
 * Actually, this is a lie. WebGl uses 4 coordinates: x, y, z and w. The above only holds if you keep w at 1.
 * After applying the vertex shader, WebGl divides all coordinates by w, yielding (x/w, y/w, z/w, 1).
 * This can be used to calculate projections - google for 'homogeneous coordinates' to find out more.
 * Compare this [site](https://www.tomdalling.com/blog/modern-opengl/explaining-homogenous-coordinates-and-projective-geometry/)
 * and the shader `basic3d.vert.glsl`.
 *
 * WebGL knows two data structures:
 *  - buffers (generic byte arrays): usually positions, normals, texture-coordinates, vertex-colors etc.
 *    buffers are accessed in shaders as 'attributes'.
 *    note that buffers contain one entry for each vertex.
 *  - textures (bitmap images).
 *
 * Shaders use these data structures in two different ways.
 *  - Attributes are values, one per vertex.
 *    For the shader, attributes are read-only.
 *    Attributes default to [0, 0, 0, 1]
 *  - Uniforms are values, one per shader.
 *    For the shader, uniforms are read-only.
 *
 * Apart from this, shaders know about two more types of data:
 *  - Varyings are values that are passed from vertex-shader to fragment-shader.
 *    They are read-only only for the fragment-shader.
 *  - Const: a compile-time constant.
 *
 * A program is just a list of compiled and linked vertex- and fragment-shaders.
 *
 *
 * Drawing: there's drawArrays and drawElements.
 *  - drawArrays is the robust all-rounder.
 *  - drawElements can be more performant if you share vertices between objects.
 *
 *
 * Rendering data is fast, but uploading it into GPU memory is slow.
 * Optimizing WebGl performance mostly means: Avoiding having GPU and CPU wait for each other.
 * The more the GPU can do in bulk, the better. The more often you have to upload data from CPU to GPU, the worse.
 *  - So avoid switching programs, buffers and uniforms if you can.
 *    (You won't be able to avoid switching buffers, because every object is likely different. But sort your objects by their shaders, and you'll save a lot of time.)
 *  - Try to do translations, rotations and shears inside the vertex-shader instead of altering the object's buffer.
 *  - If appropriate, create über-shaders and über-buffers, that contain information for more than just one object.
 *
 * There is another thing that affects performance:
 * WebGL will only run fragment-shaders when the object's pixels aren't already obscured by a larger object in front of it.
 * That means it makes sense to first draw large objects that are close to the camera - all objects behind them won't need their fragment-shader executed.
 *
 * All `create*` functions unbind variables after setting their values. This is to avoid unwanted side-effects.
 *
 *
 *
 * WebGL components
 *    - global-state
 *        - ARRAY_BUFFER_BINDING: currently bound buffer
 *        - VERTEX_ARRAY_BINDING: currently bound vertex-array (in WebGL 1 this was always only the global vertex-array, in WebGL 2 you can now create your own ones)
 *        - ACTIVE_TEXTURE: currently bound texture
 *        - texture-units: a list of pointers to texture-buffers.
 *        - uniform-buffer-bindings (WebGL2 only): a list of pointers to uniform-buffers.
 *    - vertex-array: a list of pointers to attribute-buffers (+ metadata like datatype, stride, offset etc.).
 *        - all attributes must have the same number of elements (though one attribute's elements may be vec2's, while another one's may be vec3's)
 *        - drawArray: attributes repeat elements in groups of three for drawing triangles
 *        - drawElements: the indices for the triangles are defined in ELEMENT_ARRAY_BUFFER_BINDING
 *        - WebGL 2.0: allows you to create your own vertex-arrays, whereas 1.0 always only used one global vertex-array.
 */
/**
 * Compile shader.
 */
export declare const compileShader: (gl: WebGLRenderingContext, typeBit: number, shaderSource: string) => WebGLShader;
/**
 * Note that every program *must* have one and only one vertex-shader
 * and one and only one fragment shader.
 * That means you cannot add multiple fragment-shaders in one program. Instead, either load them in consecutively as part of different programs,
 * or generate an über-shader that contains both codes.
 */
export declare const createShaderProgram: (gl: WebGLRenderingContext, vertexShaderSource: string, fragmentShaderSource: string) => WebGLProgram;
export declare const setup3dScene: (gl: WebGLRenderingContext) => void;
export declare const updateViewPort: (gl: WebGLRenderingContext, x0: number, y0: number, x1: number, y1: number) => void;
export declare const bindProgram: (gl: WebGLRenderingContext, program: WebGLProgram) => void;
export declare const clearBackground: (gl: WebGLRenderingContext, color: number[]) => void;
/**
 * A generic buffer, together with it's metadata.
 */
export interface BufferObject {
    buffer: WebGLBuffer;
    vectorSize: number;
    vectorCount: number;
    type: number;
    normalize: boolean;
    stride: number;
    offset: number;
    drawingMode: number;
}
/**
 * Create buffer. Creation is slow! Do *before* render loop.
 */
export declare const createFloatBuffer: (gl: WebGLRenderingContext, data: number[][], drawingMode?: number) => BufferObject;
export declare const drawArray: (gl: WebGLRenderingContext, bo: BufferObject) => void;
export declare const updateBufferData: (gl: WebGLRenderingContext, bo: BufferObject, newData: number[][]) => BufferObject;
/**
 * Fetch attribute's location (attribute declared in some shader). Slow! Do *before* render loop.
 */
export declare const getAttributeLocation: (gl: WebGLRenderingContext, program: WebGLProgram, attributeName: string) => number;
/**
 * Attributes vary from vertex to vertex - that means that there are *many* of them.
 * So it makes sense for WebGl to store attribute values in a dedicated data structure - the buffer.
 */
export declare const bindBufferToAttribute: (gl: WebGLRenderingContext, attributeLocation: number, bufferObject: BufferObject) => void;
export interface IndexBufferObject {
    buffer: WebGLBuffer;
    count: number;
    type: number;
    offset: number;
    drawingMode: number;
}
export declare const createIndexBuffer: (gl: WebGLRenderingContext, indices: number[][], drawingMode?: number) => IndexBufferObject;
export declare const bindIndexBuffer: (gl: WebGLRenderingContext, ibo: IndexBufferObject) => void;
export declare const drawElements: (gl: WebGLRenderingContext, ibo: IndexBufferObject) => void;
export interface TextureObject {
    texture: WebGLTexture;
    width: number;
    height: number;
    level: number;
    internalformat: number;
    format: number;
    type: number;
    border: number;
}
/**
 * A shader's attributes get their buffer-values from the VERTEX_ARRAY, but they are constructed in the ARRAY_BUFFER.
 * Textures analogously are served from the TEXTURE_UNITS, while for construction they are bound to ACTIVE_TEXTURE.
 *
 * There is a big difference, however. Contrary to buffers which receive their initial value while still outside the ARRAY_BUFFER,
 * a texture does already have to be bound into the TEXTURE_UNITS when it's being created.
 * Since it'll always be bound into the slot that ACTIVE_TEXTURE points to, you can inadvertently overwrite another texture that is
 * currently in this place. To avoid this, we provide a dedicated `textureConstructionBindPoint`.
 *
 * Buffers are easier in this, since with vertexAttribPointer we are guaranteed to get a slot in the VERTEX_ARRAY that is not
 * already occupied by another buffer.
 */
export declare const createTexture: (gl: WebGLRenderingContext, image: HTMLImageElement | HTMLCanvasElement) => TextureObject;
export declare type textureDataType = '';
/**
 * This is just another texture, but optimized for carrying data, not for display.
 *
 * Valid combinations of texture-data parameters:
 *
 * | Internal Format | Format          | Type                      | Source Bytes Per Pixel |
 * |-----------------|-----------------|---------------------------|------------------------|
 * | RGBA            | RGBA            | UNSIGNED_BYTE             | 4                      |
 * | RGB	         | RGB             | UNSIGNED_BYTE             | 3                      |
 * | RGBA            | RGBA            | UNSIGNED_SHORT_4_4_4_4    | 2                      |
 * | RGBA            | RGBA            | UNSIGNED_SHORT_5_5_5_1	   | 2                      |
 * | RGB             | RGB             | UNSIGNED_SHORT_5_6_5      | 2                      |
 * | LUMINANCE_ALPHA | LUMINANCE_ALPHA | UNSIGNED_BYTE	           | 2                      |
 * | LUMINANCE       | LUMINANCE       | UNSIGNED_BYTE             | 1                      |
 * | ALPHA           | ALPHA           | UNSIGNED_BYTE             | 1                      |
 * Plus many more in WebGL2.
 *
 */
export declare const createDataTexture: (gl: WebGLRenderingContext, data: number[][][]) => TextureObject;
export declare const createEmptyTexture: (gl: WebGLRenderingContext, width: number, height: number) => TextureObject;
/**
 * Even though we reference textures as uniforms in a fragment shader, assigning an actual texture-value to that uniform works differently from normal uniforms.
 * Normal uniforms have a concrete value.
 * Texture uniforms, on the other hand, are just an integer-index that points to a special slot in the GPU memory (the bindPoint) where the actual texture value lies.
 */
export declare const bindTextureToUniform: (gl: WebGLRenderingContext, texture: WebGLTexture, bindPoint: number, uniformLocation: WebGLUniformLocation) => void;
export declare const updateTexture: (gl: WebGLRenderingContext, to: TextureObject, newData: HTMLImageElement | HTMLCanvasElement | number[][][]) => TextureObject;
export interface FramebufferObject {
    framebuffer: WebGLFramebuffer;
    texture: TextureObject;
    width: number;
    height: number;
}
export declare const createFramebuffer: (gl: WebGLRenderingContext) => WebGLFramebuffer;
/**
 * The operations `clear`, `drawArrays` and `drawElements` only affect the currently bound framebuffer.
 *
 * Note that binding the framebuffer does *not* mean binding its texture.
 * In fact, if there is a bound texture, it must be the *input* to a shader, not the output.
 * Therefore, a framebuffer's texture must not be bound when the framebuffer is.
 */
export declare const bindFramebuffer: (gl: WebGLRenderingContext, fbo: FramebufferObject, manualViewport?: [number, number, number, number]) => void;
/**
 * Webgl renders to the viewport, which is relative to canvas.width * canvas.height.
 * (To be more precise, only *polygons* are clipped to the viewport.
 * Operations like `clearColor()` et.al., will still draw to the *full* canvas.width * height!
 * If you want to also constrain clearColor, use `scissor` instead of viewport.)
 * That canvas.width * canvas.height then gets stretched to canvas.clientWidth * canvas.clientHeight.
 * (Note: the full canvas.width gets stretched to clientWidth, not just the viewport!)
 */
export declare const bindOutputCanvasToFramebuffer: (gl: WebGLRenderingContext, manualViewport?: [number, number, number, number]) => void;
/**
 * A framebuffer can have a texture - that is the bitmap that the shader-*out*put is drawn on.
 * Shaders may also have one or more *in*put texture(s), which must be provided to the shader as a uniform sampler2D.
 * Only the shader needs to know about any potential input texture, the framebuffer will always only know about it's output texture.
 */
export declare const bindTextureToFramebuffer: (gl: WebGLRenderingContext, texture: TextureObject, fb: WebGLFramebuffer) => FramebufferObject;
/**
 * Fetch uniform's location (uniform declared in some shader). Slow! Do *before* render loop.
 */
export declare const getUniformLocation: (gl: WebGLRenderingContext, program: WebGLProgram, uniformName: string) => WebGLUniformLocation;
export declare type WebGLVariableType = 'bool' | 'bvec2' | 'bvec3' | 'bvec4' | 'bool[]' | 'bvec2[]' | 'bvec3[]' | 'bvec4[]' | 'int' | 'ivec2' | 'ivec3' | 'ivec4' | 'int[]' | 'ivec2[]' | 'ivec3[]' | 'ivec4[]' | 'float' | 'vec2' | 'vec3' | 'vec4' | 'float[]' | 'vec2[]' | 'vec3[]' | 'vec4[]' | 'mat2' | 'mat3' | 'mat4';
/**
 * Contrary to attributes, uniforms don't need to be stored in a buffer. (Note: in WebGL 2.0, however, there *are* uniform buffers!)
 *
 * 'v' is not about the shader, but how you provide data from the js-side.
 * uniform1fv(loc, [3.19]) === uniform1f(loc, 3.19)
 *
 * |js                                      |          shader                  |
 * |----------------------------------------|----------------------------------|
 * |uniform1f(loc, 3.19)                    |  uniform float u_pi;             |
 * |uniform2f(loc, 3.19, 2.72)              |  uniform vec2 u_constants;       |
 * |uniform2fv(loc, [3.19, 2.72])           |  uniform vec2 u_constants;       |
 * |uniform1fv(loc, [1, 2, 3, 4, 5, 6])     |  uniform float u_kernel[6];      |
 * |uniform2fv(loc, [1, 2, 3, 4, 5, 6])     |  uniform vec2 u_observations[3]; |
 * |uniformMatrix3fv(loc, [[...], [...]])   |  uniform mat3 u_matrix;          |
 *
 * A note about `structs`. A shader code like this:
 * ```glsl
 * struct LightInfo {
 *    vec4 Position;
 *    vec3 La;
 * };
 * uniform LightInfo Light;
 * ```
 * ... is accessed like that:
 * ```js
 * const lightPosLoc = gl.getUniformLocation(program, "Light.Position");
 * const lightLaLoc = gl.getUniformLocation(program, "Light.La");
 * gl.uniform4fv(lightPosLoc, [1, 2, 3, 4]);
 * gl.uniform3fv(lightLaLoc, [1, 2, 3]);
 * ```
 *
 */
export declare const bindValueToUniform: (gl: WebGLRenderingContext, uniformLocation: WebGLUniformLocation, type: WebGLVariableType, values: number[]) => void;
/**
 * (From https://hacks.mozilla.org/2013/04/the-concepts-of-webgl/ and https://stackoverflow.com/questions/56303648/webgl-rendering-buffers:)
 * Ignoring handmade framebuffers, WebGl has two framebuffers that are always in use: the `frontbuffer/displaybuffer` and the `backbuffer/drawingbuffer`.
 * WebGl per default renders to the `drawingbuffer`, aka. the `backbuffer`.
 * There is also the currently displayed buffer, named the `frontbuffer` aka. the `displaybuffer`.
 * the WebGL programmer has no explicit access to the frontbuffer whatsoever.
 *
 * Once you called `clear`, `drawElements` or `drawArrays`, the browser marks the canvas as `needs to be composited`.
 * Assuming `preserveDrawingBuffer == false` (the default): Immediately before compositing, the browser
 *  - swaps the back- and frontbuffer
 *  - clears the new backbuffer.
 * If `preserveDrawingBuffer === true`: Immediately before compositing, the browser
 *  - copies the drawingbuffer to the frontbuffer.
 *
 * As a consequence, if you're going to use canvas.toDataURL or canvas.toBlob or gl.readPixels or any other way of getting data from a WebGL canvas,
 * unless you read it in the same event then it will likely be clear when you try to read it.
 *
 * In the past, old games always preserved the drawing buffer, so they'd only have to change those pixels that have actually changed. Nowadays preserveDrawingBuffer is false by default.
 *
 * A (almost brutal) workaround to get the canvas to preserve the drawingBuffer can be found here: https://stackoverflow.com/questions/26783586/canvas-todataurl-returns-blank-image
 */
export declare const getCurrentFramebuffersPixels: (canvas: HTMLCanvasElement) => ArrayBuffer;
export declare const getDebugInfo: (gl: WebGLRenderingContext) => object;
